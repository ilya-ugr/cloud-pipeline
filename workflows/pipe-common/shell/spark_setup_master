#!/usr/bin/env bash

# Copyright 2017-2019 EPAM Systems, Inc. (https://www.epam.com/)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

function get_proxy_value {
    local proxy_var_name="$1"

    # Try to get the proxy value in the lower case (e.g. http_proxy)
    proxy_var_name="$(echo $proxy_var_name | tr '[:upper:]' '[:lower:]')"
    local proxy_var_value="${!proxy_var_name}"
    [ "$proxy_var_value" ] && echo "$proxy_var_value" && return

    # If a lower case variable has no value - try an upper case (e.g. HTTP_PROXY)
    proxy_var_name="$(echo $proxy_var_name | tr '[:lower:]' '[:upper:]')"
    echo "${!proxy_var_name}"
}

function get_proxy_for_java {
    local proxy_type="$1"
    
    local proxy_var_name="${proxy_type}_proxy"
    local proxy_value="$(get_proxy_value $proxy_var_name)"
    [ -z "$proxy_value" ] && return

    local proxy_host_name="$(parse_url $proxy_value hostname)"
    local proxy_port="$(parse_url "$proxy_value" port)"
    ([ -z "$proxy_host_name" ] || [ -z "$proxy_port" ]) && return

    echo "-D$proxy_type.proxyHost=$proxy_host_name -D$proxy_type.proxyPort=$proxy_port"
}

function get_spark_workers_count {
    local api_endpoint="$1"

    local workers_to_skip=1
    [ "$CP_CAP_SPARK_MASTER_NO_SCHEDULE" == "true" ] && workers_to_skip=0

    local workers_count=$(( $(curl -s $api_endpoint | jq -r '.aliveworkers') - $workers_to_skip ))
    (( "$workers_count" < 0 )) && workers_count=0
    echo "$workers_count"
}

function setup_executors {
    local spark_config_file="$1"

    nodes_total=$(( ${node_count:-0} + 1 ))

    mem_per_node=$(grep MemTotal /proc/meminfo | awk '{print $2}')
    mem_per_node=$(( $mem_per_node / 1024 / 1024 ))
    mem_total=$(( $mem_per_node * $nodes_total ))

    cpu_per_node=$(nproc)
    cpu_total=$(( $cpu_per_node * $nodes_total ))

    executor_cores=${CP_CAP_SPARK_EXECUTOR_CORES:-2}
    executors_num=$(( $cpu_total / $executor_cores ))
    executor_mem=$(( ($mem_total / $executors_num) - 1 ))

    if grep -q "spark.executor.cores" "$spark_config_file"  2>/dev/null; then
        pipe_log_warn "Spark config file at $spark_config_file already contains executors configuration, if WILL NOT be updated" "$SPARK_MASTER_SETUP_TASK"
    else
        cat >> "$spark_config_file" << EOF
spark.executor.cores        $executor_cores
spark.executor.memory       ${executor_mem}G
EOF
    fi
}

# FIXME: this sets up access only to S3 and only for "current" region (which is used to run a compute node)
# FIXME: such an implementation shall be treated as a PoC and further replaced with the temporary tokens for all the supported Cloud Providers
function setup_object_storage_access {
    local spark_config_file="$1"

    pipe_log_info "Starting Object Storage access configuration" "$SPARK_MASTER_SETUP_TASK"

    if [ -z "$CLOUD_REGION_ID" ]; then
        pipe_log_fail "Current region ID is not set via CLOUD_REGION_ID, object storages access WILL NOT be configured" "$SPARK_MASTER_SETUP_TASK"
        return 1
    fi

    local key_id_var_name="CP_ACCOUNT_ID_${CLOUD_REGION_ID}"
    local access_key_var_name="CP_ACCOUNT_KEY_${CLOUD_REGION_ID}"
    local region_var_name="CP_ACCOUNT_REGION_${CLOUD_REGION_ID}"

    export AWS_ACCESS_KEY_ID="${!key_id_var_name}"
    export AWS_SECRET_ACCESS_KEY="${!access_key_var_name}"
    export AWS_DEFAULT_REGION="${!region_var_name}"

    if [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_ACCESS_KEY_ID" ]; then
        unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_DEFAULT_REGION
        pipe_log_fail "Credentials for the Cloud Region $CLOUD_REGION_ID cannot be found, object storages access WILL NOT be configured" "$SPARK_MASTER_SETUP_TASK"
        return 1
    fi

    if grep -q "spark.hadoop.fs.s3a" "$spark_config_file"  2>/dev/null; then
        pipe_log_warn "Spark config file at $spark_config_file already contains S3A configuration, if WILL NOT be updated" "$SPARK_MASTER_SETUP_TASK"
    else
        cat >> "$spark_config_file" << EOF
spark.hadoop.fs.s3a.impl                                org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint                            s3.${AWS_DEFAULT_REGION}.amazonaws.com
spark.hadoop.fs.s3a.server-side-encryption-algorithm    AES256
spark.executor.extraJavaOptions                         -Dcom.amazonaws.services.s3.enableV4=true
spark.driver.extraJavaOptions                           -Dcom.amazonaws.services.s3.enableV4=true
EOF
    fi

    spark_env_file="$CP_CAP_SPARK_INSTALL_DIR/conf/spark-env.sh"
    if grep -q "AWS_" "$spark_env_file" 2>/dev/null; then
        pipe_log_warn "Spark environment file at $spark_env_file already contains S3A configuration, if WILL NOT be updated" "$SPARK_MASTER_SETUP_TASK"
    else
        cat >> "$spark_env_file" << EOF
AWS_ACCESS_KEY_ID="${!key_id_var_name}"
AWS_SECRET_ACCESS_KEY="${!access_key_var_name}"
EOF
    fi

    pipe_log_info "Object Storage access configuration is done" "$SPARK_MASTER_SETUP_TASK"
    return 0
}

function setup_proxy_config {
    local spark_config_file="$1"

    java_proxy_options="$(get_proxy_for_java http)"
    java_proxy_options="$java_proxy_options $(get_proxy_for_java https)"
    java_proxy_options="$java_proxy_options $(get_proxy_for_java ftp)"

    # Translate no_proxy shell to java notation:
    # , -> | (delimiter)
    # . -> *. (wildcard domain name)
    # e.g.: "127.0.0.1,localhost,.local" -> "127.0.0.1|localhost|*.local"
    java_no_proxy="$(get_proxy_value no_proxy)"
    if [ "$java_no_proxy" ]; then
        java_no_proxy="$(echo "$java_no_proxy" | sed 's/,/|/g' | sed 's/|\./|*./g')"
        java_proxy_options="$java_proxy_options -Dhttp.nonProxyHosts=$java_no_proxy -Dftp.nonProxyHosts=$java_no_proxy"
    fi

    # Write the proxy options only if the java_proxy_options is not empty or does not consist of empty spaces
    if [ "${java_proxy_options// }" ]; then
        [ -f "$spark_config_file" ] && sed -i '/spark.driver.extraJavaOptions/d' "$spark_config_file"
        echo "spark.driver.extraJavaOptions $java_proxy_options" >> "$spark_config_file"
    fi
}

SPARK_MASTER_SETUP_TASK="SparkMasterSetup"
SPARK_MASTER_SETUP_TASK_WORKERS="SparkMasterSetupWorkers"

export CP_CAP_SPARK_INSTALL_DIR="${CP_CAP_SPARK_INSTALL_DIR:-/common/spark}"
export CP_CAP_SPARK_VERSION="${CP_CAP_SPARK_VERSION:-2.4.3}"
export CP_CAP_SPARK_DIST_URL="${CP_CAP_SPARK_DIST_URL:-https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/spark/spark-$CP_CAP_SPARK_VERSION.tgz}"
export CP_CAP_SPARK_HOST="${CP_CAP_SPARK_HOST:-$(hostname)}"
export CP_CAP_SPARK_PORT="${CP_CAP_SPARK_PORT:-7077}"
export CP_CAP_SPARK_UI_PORT="${CP_CAP_SPARK_UI_PORT:-8088}"

rm -rf "$CP_CAP_SPARK_INSTALL_DIR"
mkdir -p "$(dirname $CP_CAP_SPARK_INSTALL_DIR)"

# Install Spark master
pipe_log_info "Installing Spark $CP_CAP_SPARK_VERSION using $CP_CAP_SPARK_DIST_URL" "$SPARK_MASTER_SETUP_TASK"

install_tmp="$(mktemp -d)"

cd $install_tmp && \
curl -s "$CP_CAP_SPARK_DIST_URL" -o spark.tar.gz && \
tar -zxf spark.tar.gz && \
rm -f spark.tar.gz && \
mv spark* "$CP_CAP_SPARK_INSTALL_DIR"

if [ $? -ne 0 ]; then
    pipe_log_fail "Spark installation failed, aborting" "$SPARK_MASTER_SETUP_TASK"
    exit 1
fi

export SPARK_HOME="$CP_CAP_SPARK_INSTALL_DIR"
if [ -f "$CP_ENV_FILE_TO_SOURCE" ]; then
    echo "export SPARK_HOME=$SPARK_HOME" >> "$CP_ENV_FILE_TO_SOURCE"
    echo "export PATH=\$PATH:$CP_CAP_SPARK_INSTALL_DIR/bin" >> "$CP_ENV_FILE_TO_SOURCE"
fi

pipe_log_info "Spark installed into $CP_CAP_SPARK_INSTALL_DIR" "$SPARK_MASTER_SETUP_TASK"

# Check jq is installed (used to parse the Spark API responses)
if ! jq --version > /dev/null 2>&1; then
    pipe_log_info "Installing jq"
    wget -q "https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64" -O /usr/bin/jq
    chmod +x /usr/bin/jq
fi

# Install java
if ! java -version > /dev/null 2>&1; then
    export CP_CAP_SPARK_JAVA_VERSION="${CP_CAP_SPARK_JAVA_VERSION:-1.8.0_222}"
    export CP_CAP_SPARK_JAVA_DIST_URL="${CP_CAP_SPARK_JAVA_DIST_URL:-https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/java/openjdk-${CP_CAP_SPARK_JAVA_VERSION}_linux-x64_bin.tar.gz}"

    pipe_log_info "JAVA not found, it will be installed using $CP_CAP_SPARK_JAVA_DIST_URL" "$SPARK_MASTER_SETUP_TASK"

    export JAVA_HOME="$CP_CAP_SPARK_INSTALL_DIR/jdk"
    export PATH="$JAVA_HOME/bin:$PATH"
    
    cd $install_tmp && \
    curl -s "$CP_CAP_SPARK_JAVA_DIST_URL" -o openjdk.tar.gz && \
    tar -zxf openjdk.tar.gz && \
    rm -f openjdk.tar.gz && \
    mv *jdk* "$JAVA_HOME"

    if [ $? -ne 0 ]; then
        pipe_log_fail "JAVA installation failed, aborting Spark installation" "$SPARK_MASTER_SETUP_TASK"
        exit 1
    fi

    if ! java -version > /dev/null 2>&1; then
        pipe_log_fail "JAVA installed without errors, but 'java -version' failed, aborting Spark installation" "$SPARK_MASTER_SETUP_TASK"
        exit 1
    fi

    if [ -f "$CP_ENV_FILE_TO_SOURCE" ]; then
        echo "export JAVA_HOME=$JAVA_HOME" >> "$CP_ENV_FILE_TO_SOURCE"
        echo "export PATH=\$PATH:$JAVA_HOME/bin" >> "$CP_ENV_FILE_TO_SOURCE"
    fi

    pipe_log_info "JAVA installed into $JAVA_HOME" "$SPARK_MASTER_SETUP_TASK"
fi

# Configuration file setup
spark_config_file="$CP_CAP_SPARK_INSTALL_DIR/conf/spark-defaults.conf"

# Setup proxies for Spark, if defined in the environment variables
setup_proxy_config "$spark_config_file"

# Setup Object Storages access for Spark (if not restricted)
if [ "$CP_CAP_SPARK_NO_OBJECT_STORAGES" == "true" ]; then
    pipe_log_info " Object Storage access configuration WILL NO be performed (prohibited by CP_CAP_SPARK_NO_OBJECT_STORAGES=true)" "$SPARK_MASTER_SETUP_TASK"
else
    setup_object_storage_access "$spark_config_file"
fi

# Setup executors num/cores/mem configuration
setup_executors "$spark_config_file"

# Start Spark master
spark_master_url="spark://${CP_CAP_SPARK_HOST}:${CP_CAP_SPARK_PORT}"
pipe_log_info "Starting Spark master service $spark_master_url with Web UI on port $CP_CAP_SPARK_UI_PORT" "$SPARK_MASTER_SETUP_TASK"
$CP_CAP_SPARK_INSTALL_DIR/sbin/start-master.sh -h "$CP_CAP_SPARK_HOST" \
                                               -p "$CP_CAP_SPARK_PORT" \
                                               --webui-port "$CP_CAP_SPARK_UI_PORT"
if [ $? -ne 0 ]; then
    pipe_log_fail "Spark master startup failed, aborting. Please review the logs in $CP_CAP_SPARK_INSTALL_DIR/logs" "$SPARK_MASTER_SETUP_TASK"
    exit 1
fi

# Add the master's URL to the config
[ -f "$spark_config_file" ] && sed -i '/spark.master/d' "$spark_config_file"
echo "spark.master $spark_master_url" >> "$spark_config_file"

pipe_log_success "Spark master is started" "$SPARK_MASTER_SETUP_TASK"

# Add a current master as a worker (if not restricted)
if [ "$CP_CAP_SPARK_MASTER_NO_SCHEDULE" == "true" ]; then
    pipe_log_info "Spark master host WILL NOT be added as a worker (prohibited by CP_CAP_SPARK_MASTER_NO_SCHEDULE=true)" "$SPARK_MASTER_SETUP_TASK"
else
    pipe_log_info "Adding Spark master host as a worker" "$SPARK_MASTER_SETUP_TASK"
    spark_setup_worker "$CP_CAP_SPARK_HOST"
    if [ $? -ne 0 ]; then
        pipe_log_fail "Cannot add a Spark master host as a worker, treating this as critical error and aborting" "$SPARK_MASTER_SETUP_TASK"
        exit 1
    fi
fi

# Wait for worker nodes to initiate and connect to the master
if [ -z "$node_count" ] || (( "$node_count" == 0 )); then
    pipe_log_success "Worker nodes count is not defined. Won't wait for them" "$SPARK_MASTER_SETUP_TASK_WORKERS"
else
    _MASTER_API_ENDPOINT="http://$CP_CAP_SPARK_HOST:$CP_CAP_SPARK_UI_PORT/json/"
    _MASTER_EXEC_WAIT_ATTEMPTS=${_MASTER_EXEC_WAIT_ATTEMPTS:-60}
    _MASTER_EXEC_WAIT_SEC=${_MASTER_EXEC_WAIT_SEC:-10}
    _CURRENT_WORKERS_COUNT=$(get_spark_workers_count "$_MASTER_API_ENDPOINT")
    while [ "$node_count" -gt "$_CURRENT_WORKERS_COUNT" ]; do
        pipe_log_info "Waiting for workers to connect. $_CURRENT_WORKERS_COUNT out of $node_count are ready" "$SPARK_MASTER_SETUP_TASK_WORKERS"
        sleep $_MASTER_EXEC_WAIT_SEC
        _CURRENT_WORKERS_COUNT=$(get_spark_workers_count "$_MASTER_API_ENDPOINT")
        _MASTER_EXEC_WAIT_ATTEMPTS=$(( _MASTER_EXEC_WAIT_ATTEMPTS-1 ))

        if (( $_MASTER_EXEC_WAIT_ATTEMPTS <= 0 )); then
            pipe_log_success "NOT all execution hosts are connected. But we are giving up waiting as threshold has been reached" "$SPARK_MASTER_SETUP_TASK_WORKERS"
            exit 0
        fi
    done
    pipe_log_success "All workers are connected" "$SPARK_MASTER_SETUP_TASK_WORKERS"
fi
